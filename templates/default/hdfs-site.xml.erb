<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!-- Put site-specific property overrides in this file. -->

<configuration>

  <property>
    <name>dfs.replication</name>
    <value><%= node['hops']['num_replicas'] %></value>
    <description>Default block replication.
    The actual number of replications can be specified when the file is created.
    The default is used if replication is not specified in create time.
    </description>
  </property>

  <property>
    <name>dfs.namenode.rpc-address</name>
    <value><%= @nn_rpc_address %></value>
    <description>
      RPC address that handles all clients requests. In the case of HA/Federation where multiple namenodes exist,
      the name service id is added to the name e.g. dfs.namenode.rpc-address.ns1
      dfs.namenode.rpc-address.EXAMPLENAMESERVICE
      The value of this property will take the form of hdfs://nn-host1:rpc-port.
    </description>
  </property>

  <property>
    <name>dfs.namenode.http-address</name>
    <value><%= @nn_http_address %></value>
    <description>The address and the base port where the dfs namenode web ui will listen on. </description>
  </property>

  <property>
    <name>dfs.client.max.retries.on.failure</name>
    <value>1</value>
  </property>

  <property>
    <name>dfs.client.block.write.locateFollowingBlock.retries</name>
    <value>10</value>
  </property>

  <!-- Do not modify this file directly.  Instead, copy entries that you -->
  <!-- wish to modify from this file into hdfs-site.xml and change them -->
  <!-- there.  If hdfs-site.xml does not already exist, create it.      -->

  <!-- <property> -->
  <!--   <name>hadoop.hdfs.configuration.version</name> -->
  <!--   <value>1</value> -->
  <!--   <description>version of this configuration file</description> -->
  <!-- </property> -->

  <property>
    <name>dfs.namenode.accesstime.precision</name>
    <value>3600000</value>
    <description>The access time for HDFS file is precise upto this value.
    The default value is 1 hour. Setting a value of 0 disables
    access times for HDFS.
    </description>
  </property>


  <property>
    <name>dfs.namenode.handler.count</name>
    <value><%= node['hops']['nn']['handler_count'] %></value>
    <description>The RPC server that listens to requests from clients</description>
  </property>

  <!-- <property> -->
  <!--    <name>dfs.namenode.service.handler.count</name> -->
  <!--   <value>10</value> -->
  <!--   <description>The RPC server threads that listens to requests from DataNodes</description> -->
  <!-- </property> -->

  <!-- <property> -->
  <!--   <name>dfs.datanode.data.dir</name> -->
  <!--   <value>file://<%= node['hops']['dn']['data_dir'] %></value> -->
  <!--   <description>Determines where on the local filesystem an DFS data node should store its blocks.  If this is a comma-delimited -->
  <!--   list of directories, then data will be storned in all named  directories, typically on different devices. Directories that do not exist are ignored. -->
  <!--   </description> -->
  <!-- </property> -->

  <property>
    <name>dfs.namenode.inodeid.batchsize</name>
    <value><%= node['hops']['dfs']['inodeid']['batchsize'] %></value>
    <description></description>
  </property>

  <property>
    <name>dfs.namenode.blockid.batchsize</name>
    <value><%= node['hops']['dfs']['blockid']['batchsize'] %></value>
    <description></description>
  </property>

  <property>
    <name>dfs.blocksize</name>
    <value><%= node['hops']['hdfs']['blocksize'] %></value>
    <description>
        The default block size for new files, in bytes.
        You can use the following suffix (case insensitive):
        k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.),
        Or provide complete size in bytes (such as 134217728 for 128 MB).
    </description>
  </property>

  <property>
    <name>dfs.client.refresh.namenode.list</name>
    <value>60000</value>
    <description>Time in ms</description>
  </property>


  <property>
    <name>dfs.namenode.name.dir</name>
    <value><%= node['hops']['nn']['name_dir'] %></value>
  </property>

  <property>
    <name>dfs.datanode.data.dir</name>
    <value><%= node['hops']['dn']['data_dir'] %></value>
  </property>

  <property>
    <name>dfs.datanode.data.dir.perm</name>
    <value><%= node['hops']['dn']['data_dir_permissions'] %></value>
  </property>

  <property>
    <name>dfs.namenode.safemode.extension</name>
    <value>30000</value>
    <description>
      Determines extension of safe mode in milliseconds
      after the threshold level is reached.
    </description>
  </property>

  <property>
    <name>dfs.namenode.processReport.batchsize</name>
    <value><%= node['hops']['dfs']['processReport']['batchsize'] %></value>
    <description>This is the number of blocks to be processed in one transaction.
    Higher values can result in higher throughput, but too high values can cause transactions to fail.
    </description>
  </property>

  <property>
    <name>dfs.namenode.misreplicated.batchsize</name>
    <value><%= node['hops']['dfs']['misreplicated']['batchsize'] %></value>
    <description>

    </description>
  </property>

  <property>
    <name>dfs.namenode.misreplicated.noofbatches</name>
    <value><%= node['hops']['dfs']['misreplicated']['noofbatches'] %></value>
    <description>
    </description>
  </property>

  <!-- <property> -->
  <!--   <name>dfs.namenode.logging.level</name> -->
  <!--   <value><%= node['hops']['log_level'] %></value> -->
  <!--   <description> -->
  <!--     The logging level for dfs namenode. Other values are "dir" (trace -->
  <!--     namespace mutations), "block" (trace block under/over replications -->
  <!--     and block creations/deletions), or "all". -->
  <!--   </description> -->
  <!-- </property> -->


  <property>
    <name>dfs.namenode.selector-policy</name>
    <value>RANDOM_STICKY</value>
    <description>Used by clients. Possible values ROUND_ROBIN, RANDOM, RANDOM_STICKY</description>
  </property>


  <property>
    <name>dfs.resolvingcache.memcache.connectionpool.size</name>
    <value>10</value>
    <description>should be same size as rpc threaads</description>
  </property>

  <property>
    <name>dfs.resolvingcache.enabled</name>
    <value><%= node['hops']['nn']['cache'] %></value>
    <description></description>
  </property>

  <property>
    <name>dfs.resolvingcache.type</name>
    <value>InMemory</value>
    <description></description>
  </property>

  <property>
    <name>dfs.resolvingcache.inmemory.maxsize</name>
    <value>2000000</value>
    <description></description>
  </property>

  <property>
    <name>dfs.ndb.setpartitionkey.enabled</name>
    <value><%= node['hops']['nn']['partition_key'] %></value>
    <description></description>
  </property>

  <property>
    <name>dfs.ndb.setrandompartitionkey.enabled</name>
    <value>true</value>
    <description></description>
  </property>

  <property>
    <name>dfs.namenode.quota.enabled</name>
    <value><%= node['hops']['hdfs']['quota_enabled'] %></value>
    <description></description>
  </property>

  <property>
    <name>dfs.permissions.enabled</name>
    <value>true</value>
    <description></description>
  </property>

  <property>
    <name>fs.permissions.umask-mode</name>
    <value><%= node['hops']['hdfs']['umask'] %></value>
    <description></description>
  </property>


  <property>
    <name>dfs.ndc.enable</name>
    <value>false</value>
    <description></description>
  </property>

  <property>
    <name>dfs.transaction.stats.enabled</name>
    <value>false</value>
    <description></description>
  </property>

  <property>
    <name>dfs.transaction.stats.detailed.enabled</name>
    <value>false</value>
    <description></description>
  </property>

  <property>
    <name>dfs.transaction.stats.writerround</name>
    <value>30</value>
    <description></description>
  </property>

  <property>
    <name>dfs.transaction.stats.dir</name>
    <value>/tmp/hopsstats</value>
    <description></description>
  </property>

  <property>
    <name>dfs.heartbeat.interval</name>
    <value>3</value>
    <description></description>
  </property>

  <!-- Enable erasure coding to test all code paths -->

  <property>
    <name>dfs.erasure_coding.enabled</name>
    <value><%= node['hops']['erasure_coding'] %></value>
    <description>Enable erasure coding</description>
  </property>

  <property>
     <name>dfs.webhdfs.enabled</name>
     <value>true</value>
  </property>


  <property>
    <name>dfs.permissions.superusergroup</name>
    <value><%= node['hops']['hdfs']['user'] %></value>
    <description>The group for hdfs superusers</description>
  </property>


<!--
  <property>
    <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
    <value><%= node['hops']['reverse_dns_lookup_supported'] %></value>
  </property>

  <property>
    <name>dfs.client.read.shortcircuit</name>
    <value>true</value>
  </property>
  <property>
    <name>dfs.domain.socket.path</name>
    <value><%= node['hops']['tmp_dir'] %>/dn_socket</value>
  </property>

-->

<!-- SSL -->
<!-- Secure webhdfs by requiring TLS two-way authentication, port
     50070 should be firewalled -->
<% if node['hops']['tls']['enabled'].eql? "true" -%>
<property>
  <name>dfs.https.enable</name>
  <value><%= node['hops']['dfs']['https']['enable'] %></value>
</property>

<property>
  <name>dfs.http.policy</name>
  <value><%= node['hops']['dfs']['http']['policy'] %></value>
</property>

<property>
  <name>dfs.https.port</name>
  <value><%= node['hops']['dfs']['https']['port'] %></value>
</property>

<property>
  <name>dfs.namenode.https-address</name>
  <value><%= @nn_https_address %></value>
  <description>The address and the base port where the dfs namenode web ui will listen on. </description>
</property>

<property>
  <name>dfs.datanode.https.address</name>
  <value><%= node['hops']['dfs']['datanode']['https']['address'] %></value>
</property>

<property>
  <name>dfs.client.https.need-auth</name>
  <value>true</value>
</property>

<property>
  <name>dfs.security-actions.actor-class</name>
  <value><%= node['hops']['fs-security-actions']['actor_class'] %></value>
</property>

<property>
  <name>dfs.security-actions.x509.get-path</name>
  <value><%= node['hops']['fs-security-actions']['x509']['get-path'] %></value>
</property>
<% end %>

<!-- Store Small Files in NDB-->

<property>
   <name>dfs.store.small.files.in.db</name>
   <value><%= node['hops']['small_files']['store_in_db'] %></value>
</property>

<property>
   <name>dfs.db.file.max.size</name>
   <value><%= node['hops']['small_files']['max_size'] %></value>
</property>

<property>
   <name>dfs.db.ondisk.small.file.max.size</name>
   <value><%= node['hops']['small_files']['on_disk']['max_size']['small']  %></value>
</property>

<property>
   <name>dfs.db.ondisk.medium.file.max.size</name>
   <value><%= node['hops']['small_files']['on_disk']['max_size']['medium']  %></value>
</property>

<property>
   <name>dfs.db.ondisk.large.file.max.size</name>
   <value><%= node['hops']['small_files']['on_disk']['max_size']['large'] %></value>
</property>

<property>
   <name>dfs.db.inmemory.file.max.size</name>
   <value><%= node['hops']['small_files']['in_memory']['max_size'] %></value>
</property>

<property>
   <name>dfs.encrypt.data.transfer</name>
   <value><%= node['hops']['encrypt_data_transfer']['enabled'] %></value>
</property>

<property>
   <name>dfs.encrypt.data.transfer.algorithm</name>
   <value><%= node['hops']['encrypt_data_transfer']['algorithm'] %></value>
</property>

<!-- Block tokens should be enabled when data transfer is encrypted -->
<property>
   <name>dfs.block.access.token.enable</name>
   <value><%= node['hops']['encrypt_data_transfer']['enabled'] %></value>
</property>

<property>
   <name>dfs.namenode.enable.retrycache</name>
   <value><%= node['hops']['nn']['enable_retrycache'] %></value>
</property>

<property>
   <name>dfs.locationDomainId</name>
   <value><%= @location_domain_id %></value>
</property>

<property>
  <name>dfs.hosts.exclude</name>
  <value><%= node['hops']['conf_dir'] %>/dfs.exclude</value>
</property>

<property>
   <name>dfs.datanode.balance.max.concurrent.moves</name>
   <value><%= node['hops']['dfs']['balance']['max_concurrent_moves'] %></value>
</property>

<property>
   <name>dfs.namenode.replication.max-streams</name>
   <value><%= node['hops']['dfs']['replication']['max_streams'] %></value>
</property>

<property>
   <name>dfs.namenode.replication.max-streams-hard-limit</name>
   <value><%= node['hops']['dfs']['replication']['max_streams_hard_limit'] %></value>
</property>

<property>
   <name>dfs.namenode.replication.work.multiplier.per.iteration</name>
   <value><%= node['hops']['dfs']['replication']['work_multiplier_per_iteration'] %></value>
</property>

<property>
  <name>dfs.namenode.object-storage.enabled</name>
  <value><%= node['hops']['enable_cloud_storage'] %></value>
  <description>
    Whether to use a cloud object storage service to store files. Currently
    only AWS S3 is supported and every following configuration is assumed to be for it.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.s3.bucket</name>
  <value><%= node['hops']['aws_s3_bucket'] %></value>
  <description>
    S3 bucket to use.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.s3.bucket.region</name>
  <value><%= node['hops']['aws_s3_region'] %></value>
  <description>
    Region where the S3 bucket is located.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.s3.append-suffix</name>
  <value>*</value>
  <description>
    String to append at the end of an S3 object key to signal it is an append object. For example, test.txt* would
    be the object key for an append to the file test.txt
  </description>
</property>

<property>
  <name>dfs.client.object-storage.part-size</name>
  <value>10000000</value>
  <description>
    Size of part to upload to S3 in bytes. Default is 5MB.
  </description>
</property>

  <property>
    <name>dfs.client.object-storage.is-buffer.enabled</name>
    <value>true</value>
    <description>
      Whether to use a buffered input stream to improve latency.
    </description>
  </property>

  <property>
    <name>dfs.client.object-storage.is-buffer.size</name>
    <value>1000000000</value>
    <description>
      Buffer size in bytes to use fo the buffered input stream.
    </description>
  </property>

  <property>
    <name>dfs.client.object-storage.max-connections</name>
    <value>24</value>
    <description>
      Maximum number of HTTP connections.
    </description>
  </property>

  <property>
    <name>dfs.client.object-storage.max-threads</name>
    <value>24</value>
    <description>
      Maximum number of threads.
    </description>
  </property>

  <property>
    <name>dfs.client.object-storage.max-tasks</name>
    <value>1</value>
    <description>
      Maximum number waiting tasks.
    </description>
  </property>

  <property>
    <name>dfs.client.object-storage.keep-alive</name>
    <value>60</value>
    <description>
      Thread keep alive time.
    </description>
  </property>

<property>
  <name>dfs.namenode.object-storage.object-management.enabled</name>
  <value>false</value>
  <description>
    Determines whether object management is enabled.
    Object management refers to the execution of background processes that continuously strive to keep a consistent
    view of the file system data in the object storage service. This is done by deleting objects that are no longer
    being tracked and by performing object consolidation. Object consolidation refers to the process of merging
    several objects that make up a single file into one, renaming objects to reflect the new name of the files
    they refer to, or decrease the size of objects after file truncation.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.deletion.leader.lookup-interval</name>
  <value>3000</value>
  <description>
    Determines the lookup interval in milliseconds for S3 object deletables that are available for scheduling.
    S3 object deletables that are scheduled are meant to be polled by the assigned workers and perform the actual API
    calls to S3 to delete the objects ultimately.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.deletion.leader.schedule-timeout</name>
  <value>10000</value>
  <description>
    Determines the schedule timeout in milliseconds for S3 object deletables that have failed to be completed. After
    this timeout, an S3 object deletable that has been scheduled but not completed will be rescheduled.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.deletion.leader.fetch-size</name>
  <value>100</value>
  <description>
    Determines the number of S3 object deletables that will be fetched for scheduling by the deletion leader.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.deletion.worker.lookup-interval</name>
  <value>1000</value>
  <description>
    Determines the lookup interval in milliseconds for S3 object deletables that have been assigned to the current
    worker. Note that lookups happen after previously looked up deletables have been processed (i.e. executing
    deletables blocks the lookup loop).
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.consolidation.leader.lookup-interval</name>
  <value>3000</value>
  <description>
    Determines the lookup interval in milliseconds for files that are available for scheduling.
    Files (S3 processables) that are scheduled are meant to be polled by the assigned workers and perform the actual API
    calls to S3 to merge, rename, and/or truncate objects.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.consolidation.leader.schedule-timeout</name>
  <value>60000</value>
  <description>
    Determines the schedule timeout in milliseconds for S3 object processables that have failed to be completed. After
    this timeout, an S3 processable that has been scheduled but not completed will be rescheduled.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.consolidation.leader.fetch-size</name>
  <value>100</value>
  <description>
    Determines the number of S3 processable that will be fetched for scheduling by the consolidation leader.
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.consolidation.worker.lookup-interval</name>
  <value>1000</value>
  <description>
    Determines the lookup interval in milliseconds for S3 processables that have been assigned to the current
    worker. Note that lookups happen after previously looked up processables have been processed (i.e. executing
    processables blocks the lookup loop).
  </description>
</property>

<property>
  <name>dfs.namenode.object-storage.object-management.consolidation.worker.lookup-interval</name>
  <value>1000</value>
  <description>
    Determines the lookup interval in milliseconds for S3 processables that have been assigned to the current
    worker. Note that lookups happen after previously looked up processables have been processed (i.e. executing
    processables blocks the lookup loop).
  </description>
</property>
</configuration>
